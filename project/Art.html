<!doctype html>
<html lang="en">
<head>
<title>The Art of Efficient Reasoning: Data, Reward, and Optimization</title>
<meta name="viewport" content="width=device-width,initial-scale=1" />
<meta name="description" content="The Art of Efficient Reasoning: Data, Reward, and Optimization" />

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

<!-- for math -->
<script>
      window.MathJax = {
        tex: {
          // 行内公式：使用 $...$ 或 \(...\)
          inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
          // 块级公式：使用 $$...$$ 或 \[...\]
          displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ]
        }
      };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<style>
.relatedthumb {
  float:left; width: 200px; margin: 3px 10px 7px 0;
}
.relatedblock {
  clear: both;
  display: inline-block;
}
.bold-sc {
  font-variant: small-caps;
  font-weight: bold;
}
.cite, .citegroup {
  margin-bottom: 8px;
}
:target {
  background-color: yellow;
}
</style>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
</script>

</head>

<body class="nd-docs">

    <div class="nd-pageheader">
    <div class="container">
        <h1 class="lead">
        <nobr class="widenobr">The Art of Efficient Reasoning: Data, Reward, and Optimization</nobr>
        </h1>

        <address>
        <nobr><a href="https://wutaiqiang.github.io/">Taiqiang Wu<sup>1</sup></a></nobr>
        <nobr>Zenan Xu<sup>2</sup></nobr>
        <nobr>Bo Zhou<sup>2</sup></nobr>
        <nobr><a href="https://www.eee.hku.hk/~nwong/">Ngai Wong<sup>1</sup></a></nobr>
        <br>  
        <nobr><sup>1</sup>HKU, EEE</nobr> &nbsp
        <nobr><sup>2</sup>Tencent</nobr>
        </address>
    </div>
    </div><!-- end nd-pageheader -->

    <div class="row justify-content-center text-center">
        <p>
        <a href="https://arxiv.org/abs/2404.02657" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="../img/FRKL_paper.png" style="border:1px solid" alt="ArXiv Preprint thumbnail" data-nothumb=""><br>ArXiv<br></a>
        </p>
        <p>
        <a href="https://arxiv.org/abs/2404.02657" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="../img/FRKL_paper.png" style="border:1px solid" alt="ArXiv Preprint thumbnail" data-nothumb=""><br>Data<br></a>
        </p>
        <p>
        <a href="https://arxiv.org/abs/2404.02657" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="../img/FRKL_paper.png" style="border:1px solid" alt="ArXiv Preprint thumbnail" data-nothumb=""><br>Weight (0.6B~30B)<br></a>
        </p>
        <!-- <p>
        <a href="https://zhuanlan.zhihu.com/p/690748958" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="../img/FRKL_zhihu.png" style="border:1px solid" alt="Zhihu thumbnail" data-nothumb=""><br>Zhihu<br></a>
        </p> -->
    </div>
    
    <div class="card" style="max-width: 1020px;">
          <div class="card-block">
          <p>
          In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs, which aims to incentivize short yet accurate thinking trajectories.
          One typically way is reward shaping, which can be formualted as:
          $$reward_i:=f(c_i) \rightarrow reward:=g(c_i,l_i).$$
          The key is to set \(reward_i\) based on both correctness \(c_i\) and length \(l_i\), rather than vanilla RL which based on correctness \(c_i\) only.
          </p>
        </div><!--card-block-->
    </div><!--card-->


    <div class="card" style="max-width: 1020px;">
      <div class="card-block">

    <div class="row">
      <div class="col">
        <h2> <span style="color: #d9534f;">Q1:</span> How to evaluate a compressed LLM? Performance under low budget?</h2>
        <p>
          <span style="color: #4fd5d9;">Answer:</span> It is quite dangerous to evaluate LLM via performance under low budget. 
          Particularly when the truncation rate is high, the trained LLM tends to overfit to short output sequences. 
          Consequently, the scaling law fails, i.e., increasing the token budget yield quite maiginal performance gains.
          In contrast, the original LLM demonstrates substantial benefits from a scaled token budget. 
          Ideally, we hope the LLM can keep high ceilling performance while the output distribution shift to shorter token region.
        </p>
        <p>
          In this paper, we advocate for more fine-grained metrics, including length distribution conditioned on correctness (length for correct and incorrect rollouts) and performance across a wide spectrum of token budgets ranging from 2k to 32k.
        </p>
    </div>
    </div>


    <div class="row">
      <div class="col">
        <h2> <span style="color: #d9534f;">Q2:</span> Can LLM trained on domain A perform well at domain B?</h2>
        <p>
          <span style="color: #4fd5d9;">Answer:</span> 
          Fortunately, the learn length inductive bias can be generalized across difficulties and domains.
          We attibuted such success to the minimal perturbation on model distributions thanks to RL optimization.
        </p>
    </div>
    </div>

    </div><!--card-block-->
    </div><!--card-->




    <div class="card" style="max-width: 1020px;">
      <div class="card-block">

    <div class="row">
      <div class="col">
        <h2> <span style="color: #d9534f;">Takeaways #1: Two-stage Paradigm</span></h2>
        <p>
          <b>Stage 1: Length Adaptation.</b> The optimization of constraint satisfaction dominates this initial phase.
          Driven by the length penalty, the model rapidly adjusts its output distribution to avoid zero-reward truncation.
        </p>
        <p>
          <b>Stage 2: Reasoning Refinement.</b> Once the rollout length stabilizes within the target budget, the training enters a stationary phase regarding length, shifting focus to performance optimization.
          In this stage, the length curves plateau, demonstrating that the model has successfully adapted to the hard constraints on output length.
        </p>
    </div>
    </div>

        <div class="row">
      <div class="col">
        <h2> <span style="color: #d9534f;">Takeaways #2: Insights</span></h2>
        <p>
          <b>Insight on Data: </b> 
          The key is to ensure sufficient and effective rewards.
          Training on easier prompts allows LLMs to focus on length reduction without compromising performance.
          Larger rollout \(N\) would be better if computational resource allows.
        </p>
        <p>
          <b>Insight on Reward: </b> 
          Not penalizing overlong correct rollouts leads to higher performance, but also slightly longer outputs.
          Sampling at target length (\(L_R=L_T\)) achieves a better trade-off via avoiding the length trap.
        </p>
        <p>
          <b>Insight on Optimization: </b> 
          Appropriate staleness can accelerate convergence without harming accuracy (before 800 steps), but also introduces latent instability, manifested as rising entropy and uncontrolled length drift.
          A moderate staleness is recommended.
        </p>
    </div>
    </div>

        </div><!--card-block-->
    </div><!--card-->


</body>
</html>