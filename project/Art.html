<!doctype html>
<html lang="en">
<head>
<title>The Art of Efficient Reasoning: Data, Reward, and Optimization</title>
<meta name="viewport" content="width=device-width,initial-scale=1" />
<meta name="description" content="The Art of Efficient Reasoning: Data, Reward, and Optimization" />

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@400;500;600;700;800&amp;family=Fraunces:opsz,wght@9..144,600;9..144,700&amp;display=swap" rel="stylesheet">
<link href="style.css" rel="stylesheet">

<!-- for math -->
<script>
      window.MathJax = {
        tex: {
          // 行内公式：使用 $...$ 或 \(...\)
          inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
          // 块级公式：使用 $$...$$ 或 \[...\]
          displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ]
        }
      };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<style>
:root {
  --bg-soft: #f5f7f2;
  --surface: #ffffff;
  --ink: #102219;
  --muted: #4a5a53;
  --brand: #024638;
  --brand-soft: #e4f1ec;
  --accent: #d9534f;
  --teal: #0b8f9a;
  --ring: rgba(2, 70, 56, 0.16);
}

body.nd-docs {
  background: radial-gradient(circle at 10% -10%, #dcece6 0%, var(--bg-soft) 50%, #edf2ee 100%);
  color: var(--ink);
  font-family: 'Plus Jakarta Sans', 'Open Sans', sans-serif;
  line-height: 1.72;
}

h1, h2, h3 {
  font-family: 'Fraunces', Georgia, serif;
  letter-spacing: 0.01em;
}

.nd-pageheader {
  background:
    linear-gradient(120deg, rgba(2, 70, 56, 0.96), rgba(3, 96, 78, 0.93)),
    radial-gradient(circle at 100% 0%, rgba(255, 255, 255, 0.2), transparent 40%);
}

.nd-pageheader .lead {
  font-size: clamp(1.85rem, 2.8vw, 2.85rem);
  line-height: 1.25;
  margin-bottom: 0.9rem;
}

.nd-pageheader address {
  margin-bottom: 0;
  opacity: 0.95;
}

.content-wrap {
  max-width: 1040px;
  margin: 0 auto;
  padding: 0 14px 28px;
}

.resource-grid {
  display: grid;
  gap: 14px;
  grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
  max-width: 760px;
  margin: -1.1rem auto 1.4rem;
}

.resource-card {
  display: flex;
  align-items: center;
  gap: 12px;
  background: var(--surface);
  color: var(--ink);
  border: 1px solid #dce7e1;
  border-radius: 14px;
  padding: 12px 14px;
  text-decoration: none;
  box-shadow: 0 8px 18px rgba(16, 34, 25, 0.07);
  transition: transform 0.2s ease, box-shadow 0.2s ease, border-color 0.2s ease;
}

.resource-card:hover,
.resource-card:focus {
  text-decoration: none;
  color: var(--ink);
  transform: translateY(-2px);
  box-shadow: 0 12px 24px rgba(16, 34, 25, 0.11);
  border-color: #bfd4ca;
}

.resource-card img {
  width: 52px;
  height: 68px;
  object-fit: cover;
  border-radius: 8px;
  border: 1px solid #c2d3cc;
}

.resource-meta strong {
  display: block;
  font-size: 0.98rem;
  line-height: 1.2;
}

.resource-meta span {
  color: var(--muted);
  font-size: 0.84rem;
}

.card {
  border: 1px solid #d8e5de;
  border-radius: 16px;
  box-shadow: 0 9px 24px rgba(16, 34, 25, 0.06);
  overflow: hidden;
}

.card-block {
  padding: 1.4rem 1.5rem;
}

.intro-card .card-block {
  background: linear-gradient(180deg, #ffffff, #fbfdfb);
}

.equation-line {
  background: var(--brand-soft);
  padding: 8px 12px;
  border-left: 4px solid var(--brand);
  border-radius: 8px;
  margin: 12px 0;
}

.section-title {
  color: var(--accent);
  font-size: 1.3rem;
  margin-bottom: 0.6rem;
}

.answer {
  color: var(--teal);
  font-weight: 700;
}

.qa-card .row + .row,
.takeaways-card .row + .row {
  margin-top: 1rem;
  padding-top: 0.9rem;
  border-top: 1px solid #e6efea;
}

.takeaway-note {
  background: #f2faf6;
  border-radius: 10px;
  padding: 10px 12px;
}

.card-header {
  background: linear-gradient(120deg, #0d5a4a, #096855);
  color: #fff;
  border-bottom: 0;
  text-transform: capitalize;
}

pre.card-text {
  margin-bottom: 0;
  background: #0f2e26;
  color: #e7fff5;
  border-radius: 10px;
  padding: 14px;
  overflow: auto;
}

p {
  margin-bottom: 0.82rem;
}

.relatedthumb {
  float:left; width: 200px; margin: 3px 10px 7px 0;
}
.relatedblock {
  clear: both;
  display: inline-block;
}
.bold-sc {
  font-variant: small-caps;
  font-weight: bold;
}
.cite, .citegroup {
  margin-bottom: 8px;
}
:target {
  background-color: yellow;
}

@media (max-width: 576px) {
  .card-block {
    padding: 1.1rem 1rem;
  }

  .resource-grid {
    margin-top: -0.8rem;
  }
}
</style>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
</script>

</head>

<body class="nd-docs">

    <div class="nd-pageheader">
    <div class="container">
        <h1 class="lead">
        <nobr class="widenobr">The Art of Efficient Reasoning: Data, Reward, and Optimization</nobr>
        </h1>

        <address>
        <nobr><a href="https://wutaiqiang.github.io/">Taiqiang Wu<sup>1</sup><sup>*</sup></a></nobr>
        <nobr>Zenan Xu<sup>2</sup><sup>*</sup></nobr>
        <nobr>Bo Zhou<sup>2</sup></nobr>
        <nobr><a href="https://www.eee.hku.hk/~nwong/">Ngai Wong<sup>1</sup></a></nobr>
        <br>  
        <nobr><sup>1</sup>HKU, EEE</nobr> &nbsp;
        <nobr><sup>2</sup>Tencent</nobr> &nbsp;
        <nobr><sup>*</sup>Equal Contributions</nobr>
        </address>
    </div>
    </div><!-- end nd-pageheader -->

<div class="content-wrap">
    <div class="resource-grid">
        <a href="https://wutaiqiang.github.io/project/Efficient_Think_RL.pdf" class="resource-card" target="_blank">
          <img src="../img/art_arxiv.png" alt="ArXiv Preprint thumbnail" data-nothumb="">
          <div class="resource-meta">
            <strong>ArXiv</strong>
            <span>Paper PDF</span>
          </div>
        </a>
        <a href="https://huggingface.co/datasets/taki555/DeepScaleR-Easy" class="resource-card" target="_blank">
          <img src="../img/art_data.png" alt="Dataset thumbnail" data-nothumb="">
          <div class="resource-meta">
            <strong>Data</strong>
            <span>Hugging Face Dataset</span>
          </div>
        </a>
    </div>

<div class="card intro-card" style="max-width: 1020px; margin: 0 auto 1.5rem auto;">
      <div class="card-block">
          <p>
          In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs, which aims to incentivize short yet accurate thinking trajectories.
          One typical method is reward shaping, which can be formulated as:
          </p>
          <p class="equation-line">$$reward_i:=f(c_i) \rightarrow reward_i:=g(c_i,l_i).$$</p>
          <p>
          The key is to set \(reward_i\) based on both correctness \(c_i\) and length \(l_i\), rather than vanilla RL that relies on correctness \(c_i\) only.
          </p>
      </div><!--card-block-->
</div><!--card-->

<div class="card qa-card" style="max-width: 1020px; margin: 0 auto 1.5rem auto;">
  <div class="card-block">

    <div class="row">
      <div class="col">
        <h3 class="section-title">Q1: How to evaluate a compressed LLM under low token budgets?</h3>
        <p>
          <span class="answer">Answer:</span> It is risky to evaluate an LLM only via performance under low budgets.
          Particularly when the truncation rate is high, the trained LLM tends to overfit to short output sequences. 
          Consequently, the scaling law may fail, where increasing the token budget yields only marginal gains.
          In contrast, the original LLM demonstrates substantial benefits from a scaled token budget. 
          Ideally, we want the LLM to retain high-ceiling performance while shifting its output distribution to shorter token regions.
        </p>
        <p>
          In this paper, we advocate more fine-grained metrics, including length distributions conditioned on correctness (length for correct and incorrect rollouts) and performance across a wide token-budget spectrum from 2k to 32k.
        </p>
      </div>
    </div>

    <div class="row">
      <div class="col">
        <h3 class="section-title">Q2: Can an LLM trained on domain A transfer well to domain B?</h3>
        <p>
          <span class="answer">Answer:</span> 
          Fortunately, the learned length inductive bias can generalize across difficulties and domains.
          We attribute this to minimal perturbation of model distributions under RL optimization.
          Please check the paper for more detailed results.
        </p>
      </div>
    </div>

  </div><!--card-block-->
</div><!--card-->


<div class="card takeaways-card" style="max-width: 1020px; margin: 0 auto 1.5rem auto;">
  <div class="card-block">
      <div class="row">
        <div class="col">
          <h3 class="section-title">Takeaway #1: Two-stage Paradigm</h3>
          <p>
            <b>Stage 1: Length Adaptation.</b> The optimization of constraint satisfaction dominates this initial phase.
            Driven by the length penalty, the model rapidly adjusts its output distribution to avoid zero-reward truncation.
          </p>
          <p>
            <b>Stage 2: Reasoning Refinement.</b> Once the rollout length stabilizes within the target budget, the training enters a stationary phase regarding length, shifting focus to performance optimization.
            In this stage, the length curves plateau, demonstrating that the model has successfully adapted to the hard constraints on output length.
          </p>
          <p>
          <span style="color: #4fd5d9;">The LLM tends to satisfy length constraints first, which is much easier than correctness. </span>
          Thus the optimization trajectories can be approximated as \(max_{c_i}\left\{max_{l_i}\left[g(c_i,l_i)\right]\right\}\).
          </p>
        </div>
      </div>

      <div class="row">
        <div class="col">
          <h3 class="section-title">Takeaway #2: Insights on Training Recipes</h3>
          <p>
          We apply the truncation strategy to the DeepSeek-Distilled-Qwen-1.5B model. For rollouts sampled at \(L_R\), only those that are correct and shorter than \(L_T\) are considered positive samples.
          </p>
          <p>
            <b>Insight on Data: </b> 
            The key is to ensure sufficient and effective rewards.
            Training on easier prompts allows LLMs to focus on length reduction without compromising performance.
            Larger rollout \(N\) is better if computational resources allow.
          </p>
          <p>
            <b>Insight on Reward: </b> 
            Not penalizing overlong correct rollouts leads to higher performance, but also slightly longer outputs.
            Sampling at target length (\(L_R=L_T\)) achieves a better trade-off by avoiding the length trap.
            Typically, the positive rollouts are shorter than negative ones, which <b>implicitly</b> encourages the model to be short yet accurate.
          </p>
          <p>
            <b>Insight on Optimization: </b> 
            Appropriate staleness can accelerate convergence without harming accuracy (before 800 steps), but also introduces latent instability, manifested as rising entropy and uncontrolled length drift.
            A moderate staleness is recommended.
          </p>
        </div>
      </div>

      <div class="row">
        <div class="col">
          <h3 class="section-title">Takeaway #3: Compressed Qwen3 Models</h3>
          <p>
          We further apply our insights to Qwen3 models. The weights are open-sourced and can be found at <a href="">this repo</a>.
          </p>
          <div class="takeaway-note">
          <span class="answer">Suggestions for hyperparameters:</span>
          <ul>
            <li>Rollout \(N\): As large as possible. We recommend 24.</li>
            <li>\(L_R\) and \(L_T\): A typical value is 8k. If the output length drops too fast, please increase them.</li>
          </ul>
          </div>
        </div>
      </div>

    </div><!--card-block-->
</div><!--card-->


<!-- <h3>How to Cite</h3> -->

<div class="card" style="max-width: 1020px; margin: 0 auto 1.5rem auto;">
<h3 class="card-header">bibliography</h3>
  <div class="card-block">
  <p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
  Taiqiang Wu, Zenan Xu, Bo Zhou, Ngai Wong. "<em>The Art of Efficient Reasoning: Data, Reward, and Optimization.</em>" Arxiv.
  </p>
  </div>
<h3 class="card-header">bibtex</h3>
  <div class="card-block">
  <pre class="card-text clickselect">@article{wu2026art,
    title={The Art of Efficient Reasoning: Data, Reward, and Optimization}, 
    author={Taiqiang Wu and Zenan Xu and Bo Zhou and Ngai Wong},
  }
  </pre>
  </div>
</div> <!--card-block-->

<p></p>
</div>

</body>
</html>
